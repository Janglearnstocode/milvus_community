---
id: multimodal-semantic-search-with-images-and-text.md
title: Мультимодальный семантический поиск с использованием изображений и текста
author: Stefan Webb
date: 2025-02-3
desc: >-
  Узнайте, как создать приложение для семантического поиска с использованием
  мультимодального искусственного интеллекта, который понимает взаимосвязь между
  текстом и изображением, не ограничиваясь базовым подбором ключевых слов.
cover: >-
  assets.zilliz.com/Multimodal_Semantic_Search_with_Images_and_Text_180d89d5aa.png
tag: Engineering
tags: 'Milvus, Vector Database, Open Source, Semantic Search, Multimodal AI'
recommend: true
canonicalUrl: 'https://milvus.io/blog/multimodal-semantic-search-with-images-and-text.md'
---
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Multimodal_Semantic_Search_with_Images_and_Text_180d89d5aa.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Как люди, мы воспринимаем мир с помощью наших органов чувств. Мы слышим звуки, видим изображения, видео и текст, часто накладывая их друг на друга. Мы понимаем мир через эти многочисленные модальности и взаимосвязь между ними. Чтобы искусственный интеллект действительно соответствовал или превосходил человеческие возможности, он должен развить в себе такую же способность понимать мир через несколько линз одновременно.</p>
<p>В этом посте и сопровождающих его видео (скоро будет) и блокноте мы расскажем о недавнем прорыве в моделях, которые могут обрабатывать как текст, так и изображения вместе. Мы продемонстрируем это на примере создания приложения для семантического поиска, которое выходит за рамки простого подбора ключевых слов - оно понимает взаимосвязь между тем, что запрашивают пользователи, и визуальным контентом, который они ищут.</p>
<p>Что делает этот проект особенно интересным, так это то, что он полностью построен с использованием инструментов с открытым исходным кодом: векторной базы данных Milvus, библиотек машинного обучения HuggingFace и набора данных отзывов покупателей Amazon. Примечательно, что всего десять лет назад для создания подобного потребовались бы значительные собственные ресурсы. Сегодня эти мощные компоненты находятся в свободном доступе и могут быть объединены инновационными способами всеми, у кого есть любопытство к экспериментам.</p>
<custom-h1>Обзор</custom-h1><p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/overview_97a124bc9a.jpg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Наше мультимодальное поисковое приложение относится к типу <em>retrieve-and-rerank.</em> Если вы знакомы с <em>retrieval-augmented-generation</em> (RAG), это очень похоже, только конечным результатом является список изображений, которые были проранжированы с помощью большой языковой модели зрения (LLVM). Поисковый запрос пользователя содержит как текст, так и изображение, а целью является набор изображений, проиндексированных в векторной базе данных. Архитектура состоит из трех этапов - <em>индексирования</em>, <em>поиска</em> и <em>переранжирования</em> (сродни "генерации"), которые мы кратко излагаем по очереди.</p>
<h2 id="Indexing" class="common-anchor-header">Индексирование<button data-href="#Indexing" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Наше поисковое приложение должно иметь что-то для поиска. В нашем случае мы используем небольшое подмножество набора данных "Amazon Reviews 2023", который содержит текст и изображения из отзывов покупателей Amazon по всем типам товаров. Вы можете представить, что подобный семантический поиск, который мы создаем, будет полезным дополнением к сайту электронной коммерции. Мы используем 900 изображений и отбрасываем текст, хотя заметим, что этот блокнот может масштабироваться до production-размера при правильном развертывании базы данных и выводов.</p>
<p>Первая часть "магии" в нашем конвейере - это выбор модели встраивания. Мы используем недавно разработанную мультимодальную модель под названием <a href="https://huggingface.co/BAAI/bge-visualized">Visualized BGE</a>, которая способна встраивать текст и изображения вместе или по отдельности в одно пространство с помощью единой модели, в которой близкие точки семантически схожи. В последнее время были разработаны и другие подобные модели, например <a href="https://github.com/google-deepmind/magiclens">MagicLens</a>.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/indexing_1937241be5.jpg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>На рисунке выше показано: вложение для [изображения льва сбоку] плюс текст "вид спереди" близко к вложению для [изображения льва спереди] без текста. Одна и та же модель используется как для входов с текстом и изображением, так и для входов только с изображением (а также для входов только с текстом). <em>Таким образом, модель способна понять намерения пользователя в том, как текст запроса связан с изображением запроса.</em></p>
<p>Мы встраиваем 900 изображений товаров без соответствующего текста и храним их в векторной базе данных с помощью <a href="https://milvus.io/docs">Milvus</a>.</p>
<h2 id="Retrieval" class="common-anchor-header">Поиск<button data-href="#Retrieval" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Теперь, когда наша база данных создана, мы можем отправить запрос пользователю. Представьте, что пользователь пришел с запросом: "чехол для телефона с этим" плюс [изображение леопарда]. То есть он ищет чехлы для телефонов с принтом в виде шкуры леопарда.</p>
<p>Обратите внимание, что в тексте запроса пользователя было написано "это", а не "шкура леопарда". Наша модель встраивания должна быть способна связать слово "это" с тем, к чему оно относится, что является впечатляющим достижением, учитывая, что предыдущая итерация моделей не могла обрабатывать такие открытые инструкции. В <a href="https://arxiv.org/abs/2403.19651">статье MagicLens</a> приводятся другие примеры.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Retrieval_ad64f48e49.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Мы вставляем текст запроса и изображение вместе и выполняем поиск по сходству в нашей векторной базе данных, возвращая девять лучших совпадений. Результаты показаны на рисунке выше, вместе с запрошенным изображением леопарда. Оказывается, что первое попавшееся изображение не является наиболее релевантным запросу. Наиболее релевантным является седьмой результат - чехол для телефона с принтом леопардовой шкуры.</p>
<h2 id="Generation" class="common-anchor-header">Поколение<button data-href="#Generation" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Похоже, что наш поиск не удался, поскольку верхний результат не является наиболее релевантным. Однако мы можем исправить это с помощью повторного ранжирования. Вы можете быть знакомы с повторным ранжированием найденных элементов как важным шагом во многих конвейерах RAG. Мы используем <a href="https://huggingface.co/microsoft/Phi-3-vision-128k-instruct">Phi-3 Vision</a> в качестве модели реранжировщика.</p>
<p>Сначала мы просим LLVM сгенерировать подпись к изображению запроса. LLVM выдает:</p>
<p><em>"На изображении крупным планом показана морда леопарда с акцентом на его пятнистый мех и зеленые глаза".</em></p>
<p>Затем мы подаем эту надпись, одно изображение с девятью результатами и изображением запроса и строим текстовую подсказку, в которой просим модель переранжировать результаты, предоставив ответ в виде списка и указав причину выбора верхнего соответствия.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Generation_b016a6c26a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Результат визуализирован на рисунке выше - наиболее релевантный элемент теперь занимает первое место, а причина выбора такова:</p>
<p><em>"Наиболее подходящим является товар с леопардовой тематикой, который соответствует запросу пользователя о чехле для телефона с аналогичной тематикой".</em></p>
<p>Наш LLVM-реранжировщик смог обеспечить понимание изображений и текста и улучшить релевантность результатов поиска. <em>Интересным артефактом является то, что реранжировщик выдал только восемь результатов и отбросил один, что подчеркивает необходимость защитных ограждений и структурированного вывода.</em></p>
<h2 id="Summary" class="common-anchor-header">Резюме<button data-href="#Summary" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>В этом посте, а также в сопровождающем его видео (скоро будет) и <a href="https://github.com/milvus-io/bootcamp/blob/master/bootcamp/tutorials/quickstart/multimodal_retrieval_amazon_reviews.ipynb">блокноте</a> мы создали приложение для мультимодального семантического поиска по тексту и изображениям. Модель встраивания могла встраивать текст и изображения вместе или по отдельности в одно и то же пространство, а модель основы могла вводить текст и изображение и генерировать текст в ответ. <em>Важно отметить, что модель встраивания смогла связать намерение пользователя, выраженное в открытой инструкции, с изображением запроса и таким образом указать, как пользователь хочет, чтобы результаты относились к введенному изображению.</em></p>
<p>Это лишь часть того, что нас ждет в ближайшем будущем. Мы увидим множество применений мультимодального поиска, мультимодального понимания и рассуждения и так далее в различных модальностях: изображения, видео, аудио, молекулы, социальные сети, табличные данные, временные ряды - потенциал безграничен.</p>
<p>И в основе этих систем лежит векторная база данных, которая является внешней "памятью" системы. Milvus - отличный выбор для этой цели. Он имеет открытый исходный код, полнофункциональный (см. <a href="https://milvus.io/blog/get-started-with-hybrid-semantic-full-text-search-with-milvus-2-5.md">статью о полнотекстовом поиске в Milvus 2.5</a>) и эффективно масштабируется до миллиардов векторов с веб-трафиком и задержкой менее 100 мс. Узнайте больше в <a href="https://milvus.io/docs">документации Milvus</a>, присоединяйтесь к нашему сообществу <a href="https://milvus.io/discord">Discord</a> и надеемся увидеть вас на нашей следующей <a href="https://lu.ma/unstructured-data-meetup">встрече по неструктурированным данным</a>. До встречи!</p>
<h2 id="Resources" class="common-anchor-header">Ресурсы<button data-href="#Resources" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li><p>Блокнот: <a href="https://github.com/milvus-io/bootcamp/blob/master/bootcamp/tutorials/quickstart/multimodal_retrieval_amazon_reviews.ipynb">"Мультимодальный поиск с помощью Amazon Reviews и LLVM Reranking</a>".</p></li>
<li><p>Видео для разработчиков AWS на Youtube (скоро будет)</p></li>
<li><p><a href="https://milvus.io/docs">Документация Milvus</a></p></li>
<li><p><a href="https://lu.ma/unstructured-data-meetup">Встреча по неструктурированным данным</a></p></li>
<li><p>Модель встраивания: <a href="https://huggingface.co/BAAI/bge-visualized">Визуализированная карта модели BGE</a></p></li>
<li><p>Альтернативная модель встраивания: <a href="https://github.com/google-deepmind/magiclens">MagicLens model repo</a></p></li>
<li><p>LLVM: <a href="https://huggingface.co/microsoft/Phi-3-vision-128k-instruct">Карта модели Phi-3 Vision</a></p></li>
<li><p>Документ: "<a href="https://arxiv.org/abs/2403.19651">MagicLens: самоконтролируемый поиск изображений с помощью открытых инструкций</a>"</p></li>
<li><p>Набор данных: <a href="https://amazon-reviews-2023.github.io/">Amazon Reviews 2023</a></p></li>
</ul>

{"codeList":["pip install pymilvus --upgrade\npip install torch numpy scikit-learn pillow\n","!wget https://github.com/milvus-io/pymilvus-assets/releases/download/imagedata/reverse_image_search.zip\n!unzip -q -o reverse_image_search.zip\n","    import torch\n    from PIL import Image\n    import timm\n    from sklearn.preprocessing import normalize\n    from timm.data import resolve_data_config\n    from timm.data.transforms_factory import create_transform\n    class FeatureExtractor:\n        def __init__(self, modelname):\n            # Load the pre-trained model\n            self.model = timm.create_model(\n                modelname, pretrained=True, num_classes=0, global_pool=\"avg\"\n            )\n            self.model.eval()\n            # Get the input size required by the model\n            self.input_size = self.model.default_cfg[\"input_size\"]\n            config = resolve_data_config({}, model=modelname)\n            # Get the preprocessing function provided by TIMM for the model\n            self.preprocess = create_transform(**config)\n        def __call__(self, imagepath):\n            # Preprocess the input image\n            input_image = Image.open(imagepath).convert(\"RGB\")  # Convert to RGB if needed\n            input_image = self.preprocess(input_image)\n            # Convert the image to a PyTorch tensor and add a batch dimension\n            input_tensor = input_image.unsqueeze(0)\n            # Perform inference\n            with torch.no_grad():\n                output = self.model(input_tensor)\n            # Extract the feature vector\n            feature_vector = output.squeeze().numpy()\n            return normalize(feature_vector.reshape(1, -1), norm=\"l2\").flatten()\n","    from pymilvus import MilvusClient\n    client = MilvusClient(uri=\"example.db\")\n    if client.has_collection(collection_name=\"image_embeddings\"):\n        client.drop_collection(collection_name=\"image_embeddings\")\n\n    client.create_collection(\n        collection_name=\"image_embeddings\",\n        vector_field_name=\"vector\",\n        dimension=2048,\n        auto_id=True,\n        enable_dynamic_field=True,\n        metric_type=\"COSINE\",\n    )\n","    import os\n    from some_module import FeatureExtractor  # Replace with your feature extraction module\n    extractor = FeatureExtractor(\"resnet50\")\n    root = \"./train\"  # Path to your dataset\n    insert = True\n    if insert:\n        for dirpath, _, filenames in os.walk(root):\n            for filename in filenames:\n                if filename.endswith(\".jpeg\"):\n                    filepath = os.path.join(dirpath, filename)\n                    image_embedding = extractor(filepath)\n                    client.insert(\n                        \"image_embeddings\",\n                        {\"vector\": image_embedding, \"filename\": filepath},\n                    )\n","    from IPython.display import display\n    from PIL import Image\n    query_image = \"./search-image.jpeg\"  # The image you want to search with\n    results = client.search(\n        \"image_embeddings\",\n        data=[extractor(query_image)],\n        output_fields=[\"filename\"],\n        search_params={\"metric_type\": \"COSINE\"},\n        limit=6,  # Top-k results\n    )\n    images = []\n    for result in results:\n        for hit in result[:10]:\n            filename = hit[\"entity\"][\"filename\"]\n            img = Image.open(filename)\n            img = img.resize((150, 150))\n            images.append(img)\n    width = 150 * 3\n    height = 150 * 2\n    concatenated_image = Image.new(\"RGB\", (width, height))\n    for idx, img in enumerate(images):\n        x = idx % 5\n        y = idx // 5\n        concatenated_image.paste(img, (x * 150, y * 150))\n\n    display(\"query\")\n    display(Image.open(query_image).resize((150, 150)))\n    display(\"results\")\n    display(concatenated_image)\n"],"headingContent":"","anchorList":[{"label":"GPT-4oで誰もが一夜にしてアーティストになった","href":"Everyone-Became-an-Artist-Overnight-with-GPT-4o","type":2,"isActive":false},{"label":"GPT-4oがすべてを変える理由","href":"Why-GPT-4o-Changes-Everything","type":2,"isActive":false},{"label":"Milvusを使ってGPT-4oとプライベートデータを接続し、よりキュレーションされた画像出力を実現する。","href":"Connecting-Your-Private-Data-with-GPT-4o-Using-Milvus-for-More-Curated-Image-Outputs","type":2,"isActive":false},{"label":"次はどうなる？私の見解と議論の余地","href":"Whats-Next-My-Perspective-and-Open-for-Discussion","type":2,"isActive":false}]}
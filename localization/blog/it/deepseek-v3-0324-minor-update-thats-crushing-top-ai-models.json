{"codeList":["! pip install --upgrade pymilvus[model] openai requests tqdm\n","import os\n\nos.environ[\"DEEPSEEK_API_KEY\"] = \"***********\"\n","! wget https://github.com/milvus-io/milvus-docs/releases/download/v2.4.6-preview/milvus_docs_2.4.x_en.zip\n! unzip -q milvus_docs_2.4.x_en.zip -d milvus_docs\n","from glob import glob\n\n# Load all markdown files from the FAQ directory\ntext_lines = []\nfor file_path in glob(\"milvus_docs/en/faq/*.md\", recursive=True):\n    with open(file_path, \"r\") as file:\n        file_text = file.read()\n        # Split on headings to separate content sections\n        text_lines += file_text.split(\"# \")\n","from openai import OpenAI\n\ndeepseek_client = OpenAI(\n   api_key=\"<OPENROUTER_API_KEY>\",\n   base_url=\"https://openrouter.ai/api/v1\",\n)\n","from pymilvus import model as milvus_model\n\n# Initialize the embedding model\nembedding_model = milvus_model.DefaultEmbeddingFunction()\n\n# Test the embedding model\ntest_embedding = embedding_model.encode_queries([\"This is a test\"])[0]\nembedding_dim = len(test_embedding)\nprint(f\"Embedding dimension: {embedding_dim}\")\nprint(f\"First 10 values: {test_embedding[:10]}\")\n","from pymilvus import MilvusClient\n\n# Initialize Milvus client (using Milvus Lite for simplicity)\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\ncollection_name = \"my_rag_collection\"\n\n# Remove existing collection if it exists\nif milvus_client.has_collection(collection_name):\n    milvus_client.drop_collection(collection_name)\n\n# Create a new collection\nmilvus_client.create_collection(\n    collection_name=collection_name,\n    dimension=embedding_dim,\n    metric_type=\"IP\",  # Inner product distance\n    consistency_level=\"Strong\",  # See https://milvus.io/docs/consistency.md for details\n)\n","from tqdm import tqdm\n\n# Create embeddings for all text chunks\ndata = []\ndoc_embeddings = embedding_model.encode_documents(text_lines)\n\n# Create records with IDs, vectors, and text\nfor i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n    data.append({\"id\": i, \"vector\": doc_embeddings[i], \"text\": line})\n\n# Insert data into Milvus\nmilvus_client.insert(collection_name=collection_name, data=data)\n","Creating embeddings:   0%|          | 0/72 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCreating embeddings: 100%|██████████| 72/72 [00:00<00:00, 246522.36it/s]\n\n\n\n\n\n{'insert_count': 72, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], 'cost': 0}\n","question = \"How is data stored in milvus?\"\n\n# Search for relevant information\nsearch_res = milvus_client.search(\n    collection_name=collection_name,\n    data=embedding_model.encode_queries([question]),  # Convert question to embedding\n    limit=3,  # Return top 3 results\n    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n    output_fields=[\"text\"],  # Return the text field\n)\n\n# Examine search results\nimport json\nretrieved_lines_with_distances = [\n    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n]\nprint(json.dumps(retrieved_lines_with_distances, indent=4))\n","[\n    [\n        \" Where does Milvus store data?\\n\\nMilvus deals with two types of data, inserted data and metadata. \\n\\nInserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\\n\\nMetadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\\n\\n###\",\n        0.6572665572166443\n    ],\n    [\n        \"How does Milvus flush data?\\n\\nMilvus returns success when inserted data are loaded to the message queue. However, the data are not yet flushed to the disk. Then Milvus' data node writes the data in the message queue to persistent storage as incremental logs. If `flush()` is called, the data node is forced to write all data in the message queue to persistent storage immediately.\\n\\n###\",\n        0.6312146186828613\n    ],\n    [\n        \"How does Milvus handle vector data types and precision?\\n\\nMilvus supports Binary, Float32, Float16, and BFloat16 vector types.\\n\\n- Binary vectors: Store binary data as sequences of 0s and 1s, used in image processing and information retrieval.\\n- Float32 vectors: Default storage with a precision of about 7 decimal digits. Even Float64 values are stored with Float32 precision, leading to potential precision loss upon retrieval.\\n- Float16 and BFloat16 vectors: Offer reduced precision and memory usage. Float16 is suitable for applications with limited bandwidth and storage, while BFloat16 balances range and efficiency, commonly used in deep learning to reduce computational requirements without significantly impacting accuracy.\\n\\n###\",\n        0.6115777492523193\n    ]\n]\n","# Combine retrieved text chunks\ncontext = \"\\n\".join(\n    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n)\n\n# Define prompts for the language model\nSYSTEM_PROMPT = \"\"\"\nYou are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n\"\"\"\n\nUSER_PROMPT = f\"\"\"\nUse the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n\n<context>\n{context}\n</context>\n\n<question>\n{question}\n</question>\n\"\"\"\n\n# Generate response with DeepSeek\nresponse = deepseek_client.chat.completions.create(\n    model=\"deepseek-chat\",\n    messages=[\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_PROMPT},\n    ],\n)\n\nprint(response.choices[0].message.content)\n","In Milvus, data is stored in two main categories: inserted data and metadata.\n\n1. **Inserted Data**: This includes vector data, scalar data, and collection-specific schema. The inserted data is stored in persistent storage as incremental logs. Milvus supports various object storage backends for this purpose, such as MinIO, AWS S3, Google Cloud Storage (GCS), Azure Blob Storage, Alibaba Cloud OSS, and Tencent Cloud Object Storage (COS).\n\n2. **Metadata**: Metadata is generated within Milvus and is specific to each Milvus module. This metadata is stored in etcd, a distributed key-value store.\n\nAdditionally, when data is inserted, it is first loaded into a message queue, and Milvus returns success at this stage. The data is then written to persistent storage as incremental logs by the data node. If the `flush()` function is called, the data node is forced to write all data in the message queue to persistent storage immediately.\n"],"headingContent":"","anchorList":[{"label":"Cosa c'è di nuovo in DeepSeek v3-0324 e quanto è davvero valido?","href":"Whats-New-in-DeepSeek-v3-0324-and-How-Good-Is-It-Really","type":2,"isActive":false},{"label":"Costruire un sistema RAG con Milvus e DeepSeek v3-0324 in 5 minuti","href":"Build-a-RAG-System-with-Milvus-and-DeepSeek-v3-0324-in-5-Minutes","type":2,"isActive":false},{"label":"Confronto tra DeepSeek-V3-0324: Versione originale e versione migliorata con RAG","href":"Comparing-DeepSeek-V3-0324-Original-vs-RAG-Enhanced-Version","type":2,"isActive":false},{"label":"DeepSeek v3-0324 può sostituire i modelli di ragionamento dedicati?","href":"Can-DeepSeek-v3-0324-Replace-Dedicated-Reasoning-Models","type":2,"isActive":false},{"label":"Il futuro dei modelli di intelligenza artificiale: Sfumare il divario di ragionamento","href":"The-Future-of-AI-Models-Blurring-the-Reasoning-Divide","type":2,"isActive":false}]}